# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import torch
from torch import nn
from torch.nn import functional as F

from typing import Any, Dict, List, Tuple
from .scripts.utils import convert_points_to_disc, generate_prompt_pairs_val
from .image_encoder import ImageEncoderViT
from .swinunetr import SwinUNETR
from .mask_decoder import MaskDecoder
from .prompt_encoder import PromptEncoder
import pdb
import numpy as np
from monai.utils import (
    BlendMode,
    PytorchPadMode,
    convert_data_type,
    convert_to_dst_type,
    ensure_tuple,
    ensure_tuple_rep,
    fall_back_tuple,
    look_up_option,
    optional_import,
    pytorch_after,
)
class Sam(nn.Module):
    mask_threshold: float = 0.0
    image_format: str = "RGB"

    def __init__(
        self,
        image_encoder: SwinUNETR,
        prompt_encoder: PromptEncoder,
        mask_decoder: MaskDecoder,
    ) -> None:
        """
        SAM predicts object masks from an image and input prompts.

        Arguments:
          image_encoder (ImageEncoderViT): The backbone used to encode the
            image into image embeddings that allow for efficient mask prediction.
          prompt_encoder (PromptEncoder): Encodes various types of input prompts.
          mask_decoder (MaskDecoder): Predicts masks from the image embeddings
            and encoded prompts.
          pixel_mean (list(float)): Mean values for normalizing pixels in the input image.
          pixel_std (list(float)): Std values for normalizing pixels in the input image.
        """
        super().__init__()
        self.image_encoder = image_encoder
        self.prompt_encoder = prompt_encoder
        self.mask_decoder = mask_decoder
        self.image_embeddings = None

    def update_point_to_patch(self, patch_coords, point_coords, point_labels):
        patch_ends = [patch_coords[-3].stop, patch_coords[-2].stop, patch_coords[-1].stop]
        patch_starts = [patch_coords[-3].start, patch_coords[-2].start, patch_coords[-1].start]
        # update point coords
        patch_starts = torch.tensor(patch_starts, device=point_coords.device).unsqueeze(0).unsqueeze(0)
        patch_ends = torch.tensor(patch_ends, device=point_coords.device).unsqueeze(0).unsqueeze(0)
        # [1 N 1]
        indices = torch.logical_and(((point_coords - patch_starts) > 0).all(2), ((patch_ends - point_coords) > 0).all(2))
        # check if it's within patch coords
        point_coords = point_coords.clone() - patch_starts
        if indices.any():
            point_labels[~indices] = -1
            point_coords[~indices] = 0
        else:
            point_coords = None
            point_labels = None        
        return point_coords, point_labels
    
    def image_forward(self, input_images, keep_cache):
        """ During training, we found image embedding reusing OOM
        For inference, sliding window inferer needs to store every patch embedding which needs update.
        """
        if self.image_embeddings is None or not keep_cache:
            self.image_embeddings = self.image_encoder(input_images)
        return self.image_embeddings
    
    def clear_cache(self):
        self.image_embeddings = None

    def forward(
        self,
        input_images: List[Dict[str, Any]],
        point_coords=None,
        point_labels=None,
        class_vector=None,
        patch_coords=None,
        point_mask=None,
        masks=None,
        keep_cache=False,
        labels=None,
        label_set=None
    ) -> List[Dict[str, torch.Tensor]]:
        """ point_mask is the disc map generated by point coords
        In training, to speed up the disc generation in the iterative training, point_mask is provided.
        In inference, since point_coords are updated based on patch coords, this point_mask will be generated within sam model. 
        """
        image_embeddings = self.image_forward(input_images, keep_cache)

        if patch_coords is not None and point_coords is not None:
            """ patch_coords is passed from monai_utils.sliding_window_inferer. 
            """
            if labels is not None and label_set is not None:
                # if labels is not None, sample from labels for each patch.
                # only in validation when labels of the whole image is provided, sample points for every position
                _, point_coords, point_labels, _, _ = \
                    generate_prompt_pairs_val(labels[patch_coords], label_set, 
                                            image_size=input_images.shape[-3:],
                                            max_point=1,
                                            device=labels.device)
            else:
                point_coords, point_labels = self.update_point_to_patch(patch_coords, point_coords, point_labels)

        if patch_coords is not None and masks is not None:
            masks = masks[patch_coords]
        if point_coords is not None and point_labels is not None:
            points = (point_coords, point_labels)
            if point_mask is None:
                point_mask = convert_points_to_disc([input_images.shape[-3],input_images.shape[-2],input_images.shape[-1]], point_coords, point_labels, radius=5)
        else:
            points = None
        if points is None and class_vector is None:
            return -torch.inf + torch.zeros([1,1,input_images.shape[-3],input_images.shape[-2],input_images.shape[-1]], device=input_images.device)
        sparse_embeddings, dense_prompt_embeddings = self.prompt_encoder(points=points, class_vector=class_vector, masks=masks, points_mask=point_mask)
        low_res_masks = self.mask_decoder(
            image_embeddings=image_embeddings,
            image_pe=self.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_prompt_embeddings
        )
        return low_res_masks
